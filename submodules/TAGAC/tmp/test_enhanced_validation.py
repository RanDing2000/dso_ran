#!/usr/bin/env python3
"""
æµ‹è¯•å¢å¼ºçš„validation inferenceåŠŸèƒ½å’Œé”™è¯¯å¤„ç†æœºåˆ¶
"""

import sys
import os
from pathlib import Path
import torch
import numpy as np

# Add project root to path
project_root = Path(__file__).parent
sys.path.append(str(project_root))
sys.path.append(str(project_root / "src"))

def test_dataset_error_handling():
    """æµ‹è¯•æ•°æ®é›†é”™è¯¯å¤„ç†æœºåˆ¶"""
    print("=" * 60)
    print("æµ‹è¯•æ•°æ®é›†é”™è¯¯å¤„ç†æœºåˆ¶")
    print("=" * 60)
    
    try:
        from src.vgn.dataset_voxel import DatasetVoxel_PTV3_Scene
        
        # ä½¿ç”¨å®é™…æ•°æ®è·¯å¾„
        raw_root = Path("data_scenes/targo_dataset")
        if not raw_root.exists():
            raw_root = Path("/home/ran.ding/projects/TARGO/data/nips_data_version6/combined/targo_dataset")
        
        print(f"ä½¿ç”¨æ•°æ®æ ¹ç›®å½•: {raw_root}")
        print(f"æ•°æ®ç›®å½•å­˜åœ¨: {raw_root.exists()}")
        
        if not raw_root.exists():
            print("âŒ æ•°æ®ç›®å½•ä¸å­˜åœ¨ï¼Œè·³è¿‡æ•°æ®é›†æµ‹è¯•")
            return False
        
        # åˆ›å»ºæ•°æ®é›†
        dataset = DatasetVoxel_PTV3_Scene(
            root=raw_root,
            raw_root=raw_root,
            ablation_dataset="1_100000",  # ä½¿ç”¨å¾ˆå°çš„æ•°æ®é›†
            model_type="ptv3_scene",
            use_complete_targ=True,
            debug=True,
            logdir=Path("./test_output")
        )
        
        print(f"âœ… æ•°æ®é›†åˆ›å»ºæˆåŠŸï¼Œå¤§å°: {len(dataset)}")
        
        # æµ‹è¯•å‰5ä¸ªæ ·æœ¬
        success_count = 0
        error_count = 0
        
        for i in range(min(5, len(dataset))):
            try:
                print(f"\næµ‹è¯•æ ·æœ¬ {i}...")
                x, y, pos = dataset[i]
                
                # éªŒè¯è¿”å›æ•°æ®çš„æ ¼å¼
                print(f"  âœ… æ ·æœ¬ {i} åŠ è½½æˆåŠŸ")
                print(f"  xç±»å‹: {type(x)}, é•¿åº¦: {len(x) if hasattr(x, '__len__') else 'N/A'}")
                if hasattr(x, '__len__') and len(x) >= 4:
                    print(f"    voxel_grid: {x[0].shape if hasattr(x[0], 'shape') else type(x[0])}")
                    print(f"    targ_grid: {x[1].shape if hasattr(x[1], 'shape') else type(x[1])}")
                    print(f"    targ_pc: {x[2].shape if hasattr(x[2], 'shape') else type(x[2])}")
                    print(f"    scene_pc: {x[3].shape if hasattr(x[3], 'shape') else type(x[3])}")
                
                success_count += 1
                
            except Exception as e:
                print(f"  âŒ æ ·æœ¬ {i} åŠ è½½å¤±è´¥: {e}")
                error_count += 1
        
        print(f"\næ•°æ®é›†æµ‹è¯•ç»“æœ:")
        print(f"  æˆåŠŸ: {success_count}")
        print(f"  å¤±è´¥: {error_count}")
        
        # æ£€æŸ¥é”™è¯¯æ—¥å¿—
        error_log_file = Path("/home/ran.ding/projects/TARGO/data/set_error_scenes.txt")
        if error_log_file.exists():
            print(f"\nâœ… é”™è¯¯æ—¥å¿—æ–‡ä»¶å­˜åœ¨: {error_log_file}")
            with open(error_log_file, 'r', encoding='utf-8') as f:
                lines = f.readlines()
                print(f"é”™è¯¯è®°å½•æ€»æ•°: {len(lines)}")
                if lines:
                    print("æœ€æ–°é”™è¯¯è®°å½•:")
                    for line in lines[-3:]:
                        print(f"  {line.strip()}")
        else:
            print(f"\nğŸ“ é”™è¯¯æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨: {error_log_file}")
        
        return success_count > 0
        
    except Exception as e:
        print(f"âŒ æ•°æ®é›†æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_validation_inference_framework():
    """æµ‹è¯•validation inferenceæ¡†æ¶ï¼ˆä¸è¿è¡Œå®é™…inferenceï¼‰"""
    print("=" * 60)
    print("æµ‹è¯•Validation Inferenceæ¡†æ¶")
    print("=" * 60)
    
    try:
        # å¯¼å…¥å¿…è¦æ¨¡å—
        from scripts.train.train_targo_ptv3 import perform_validation_grasp_evaluation
        
        # åˆ›å»ºæ¨¡æ‹Ÿçš„ç½‘ç»œå’Œå‚æ•°
        class MockNet:
            def __init__(self):
                self.mock_params = torch.nn.Parameter(torch.randn(10, 10))
            
            def state_dict(self):
                return {'mock_params': self.mock_params}
        
        mock_net = MockNet()
        mock_sc_net = None
        device = torch.device('cpu')
        logdir = Path('./test_validation_output')
        epoch = 999  # Test epoch
        
        print("âœ… Mockå¯¹è±¡åˆ›å»ºæˆåŠŸ")
        print(f"Device: {device}")
        print(f"Log directory: {logdir}")
        print(f"Test epoch: {epoch}")
        
        # æ£€æŸ¥éªŒè¯æ•°æ®é›†è·¯å¾„
        ycb_test_root = 'data_scenes/ycb/maniskill-ycb-v2-slight-occlusion-1000'
        acronym_test_root = 'data_scenes/acronym/acronym-slight-occlusion-1000'
        
        print(f"\næ•°æ®é›†å¯ç”¨æ€§æ£€æŸ¥:")
        print(f"  YCB: {'âœ… å­˜åœ¨' if Path(ycb_test_root).exists() else 'âŒ ä¸å­˜åœ¨'} ({ycb_test_root})")
        print(f"  ACRONYM: {'âœ… å­˜åœ¨' if Path(acronym_test_root).exists() else 'âŒ ä¸å­˜åœ¨'} ({acronym_test_root})")
        
        print(f"\nğŸ“ æ³¨æ„: ç”±äºè¿™æ˜¯æ¡†æ¶æµ‹è¯•ï¼Œä¸ä¼šè¿è¡Œå®é™…çš„inference")
        print(f"ğŸ“ å®é™…çš„inferenceéœ€è¦å®Œæ•´çš„æ¨¡å‹å’ŒéªŒè¯æ•°æ®é›†")
        
        # ä¸å®é™…è°ƒç”¨validationå‡½æ•°ï¼Œå› ä¸ºéœ€è¦å®Œæ•´ç¯å¢ƒ
        # ycb_rate, acronym_rate, avg_rate = perform_validation_grasp_evaluation(
        #     mock_net, mock_sc_net, device, logdir, epoch
        # )
        
        print("\nâœ… Validation inferenceæ¡†æ¶æµ‹è¯•å®Œæˆ")
        print("ğŸ“‹ æ¡†æ¶åŒ…å«ä»¥ä¸‹å¢å¼ºåŠŸèƒ½:")
        print("  - è¯¦ç»†çš„è°ƒè¯•è¾“å‡ºå’Œè¿›åº¦è·Ÿè¸ª")
        print("  - æ›´å¥½çš„é”™è¯¯å¤„ç†å’Œæ¢å¤æœºåˆ¶")
        print("  - æ•°æ®é›†å¯ç”¨æ€§æ£€æŸ¥")
        print("  - ä¸´æ—¶æ–‡ä»¶çš„è‡ªåŠ¨æ¸…ç†")
        print("  - ç»“æœçš„å¤šé‡éªŒè¯å’Œæ ¼å¼åŒ–")
        print("  - ACRONYMæ•°æ®é›†çš„ä¸´æ—¶ç¦ç”¨ï¼ˆé¿å…æŸåæ•°æ®é—®é¢˜ï¼‰")
        
        return True
        
    except Exception as e:
        print(f"âŒ Validation inferenceæ¡†æ¶æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_error_logging():
    """æµ‹è¯•é”™è¯¯æ—¥å¿—åŠŸèƒ½"""
    print("=" * 60)
    print("æµ‹è¯•é”™è¯¯æ—¥å¿—åŠŸèƒ½")
    print("=" * 60)
    
    try:
        from src.vgn.dataset_voxel import ERROR_LOG_FILE, safe_specify_num_points
        
        print(f"é”™è¯¯æ—¥å¿—æ–‡ä»¶è·¯å¾„: {ERROR_LOG_FILE}")
        
        # æµ‹è¯•safe_specify_num_pointså‡½æ•°çš„é”™è¯¯å¤„ç†
        print("\næµ‹è¯•ç©ºç‚¹äº‘å¤„ç†...")
        
        # æµ‹è¯•ç©ºæ•°ç»„
        empty_points = np.array([]).reshape(0, 3)
        result = safe_specify_num_points(empty_points, 100, "test_scene_empty", "test_points")
        print(f"ç©ºç‚¹äº‘å¤„ç†ç»“æœ: {result}")
        
        # æµ‹è¯•æœ‰æ•ˆç‚¹äº‘
        valid_points = np.random.rand(50, 3)
        result = safe_specify_num_points(valid_points, 25, "test_scene_valid", "test_points")
        print(f"æœ‰æ•ˆç‚¹äº‘å¤„ç†ç»“æœ: {result.shape if result is not None else None}")
        
        # æ£€æŸ¥é”™è¯¯æ—¥å¿—æ˜¯å¦è®°å½•äº†é”™è¯¯
        if ERROR_LOG_FILE.exists():
            print(f"\nâœ… é”™è¯¯æ—¥å¿—åŠŸèƒ½æ­£å¸¸")
            with open(ERROR_LOG_FILE, 'r', encoding='utf-8') as f:
                lines = f.readlines()
                print(f"æ€»é”™è¯¯è®°å½•: {len(lines)}")
                
                # æŸ¥æ‰¾æµ‹è¯•ç›¸å…³çš„é”™è¯¯
                test_errors = [line for line in lines if 'test_scene' in line]
                print(f"æµ‹è¯•é”™è¯¯è®°å½•: {len(test_errors)}")
                for error in test_errors[-2:]:
                    print(f"  {error.strip()}")
        else:
            print(f"ğŸ“ é”™è¯¯æ—¥å¿—æ–‡ä»¶å°šä¸å­˜åœ¨")
        
        return True
        
    except Exception as e:
        print(f"âŒ é”™è¯¯æ—¥å¿—æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    print("ğŸ§ª å¢å¼ºValidationåŠŸèƒ½æµ‹è¯•å¥—ä»¶")
    print("=" * 80)
    
    all_tests_passed = True
    
    # æµ‹è¯•1: æ•°æ®é›†é”™è¯¯å¤„ç†
    test1_result = test_dataset_error_handling()
    all_tests_passed = all_tests_passed and test1_result
    
    print("\n" + "="*80)
    
    # æµ‹è¯•2: Validation inferenceæ¡†æ¶
    test2_result = test_validation_inference_framework()
    all_tests_passed = all_tests_passed and test2_result
    
    print("\n" + "="*80)
    
    # æµ‹è¯•3: é”™è¯¯æ—¥å¿—åŠŸèƒ½
    test3_result = test_error_logging()
    all_tests_passed = all_tests_passed and test3_result
    
    # æœ€ç»ˆç»“æœ
    print("\n" + "="*80)
    print("ğŸ“Š æµ‹è¯•ç»“æœæ€»ç»“")
    print("="*80)
    print(f"æ•°æ®é›†é”™è¯¯å¤„ç†: {'âœ… é€šè¿‡' if test1_result else 'âŒ å¤±è´¥'}")
    print(f"Validationæ¡†æ¶: {'âœ… é€šè¿‡' if test2_result else 'âŒ å¤±è´¥'}")
    print(f"é”™è¯¯æ—¥å¿—åŠŸèƒ½: {'âœ… é€šè¿‡' if test3_result else 'âŒ å¤±è´¥'}")
    print(f"\næ€»ä½“ç»“æœ: {'ğŸ‰ å…¨éƒ¨é€šè¿‡' if all_tests_passed else 'âš ï¸  éƒ¨åˆ†å¤±è´¥'}")
    
    if all_tests_passed:
        print("\nğŸš€ å¢å¼ºçš„validationåŠŸèƒ½å·²å‡†å¤‡å°±ç»ª!")
        print("ğŸ’¡ å¯ä»¥è¿è¡Œä»¥ä¸‹å‘½ä»¤å¼€å§‹è®­ç»ƒ:")
        print("python scripts/train_targo_ptv3.py --use_complete_targ --ablation_dataset 1_100000 --epochs 3")
    else:
        print("\nğŸ”§ è¯·æ£€æŸ¥å¤±è´¥çš„æµ‹è¯•å¹¶ä¿®å¤ç›¸å…³é—®é¢˜")
        sys.exit(1) 